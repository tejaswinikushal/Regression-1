{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f5c25d",
   "metadata": {},
   "source": [
    "#### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc811f8",
   "metadata": {},
   "source": [
    "##### 1.Simple Linear Regression:\n",
    "It is used to model a relationship between two continuous variables.i.e;one is input variable and the other is output variable.\n",
    "\n",
    "EX:Suppose a company wants to determine relation between number of hours employee spend on training and spend on work.\n",
    "\n",
    "##### 2.Multiple Linear Regression:\n",
    "It is used to find mathematical relationship between several random variables .i.e;multiple independent variables and single output variable.\n",
    "\n",
    "EX:Suppose to determine a price of house we need two features i.e;area of house ,number of rooms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc085e61",
   "metadata": {},
   "source": [
    "#### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d561bf",
   "metadata": {},
   "source": [
    "Linearity: The relationship between the independent variable(s) and the dependent variable is assumed to be linear. You can check this assumption by plotting the data and examining the scatter plot. If the relationship appears to be approximately linear, the assumption is likely met.\n",
    "\n",
    "Independence: The residuals (the differences between the observed and predicted values) should be independent of each other\n",
    "\n",
    "\n",
    "Normality of Residuals: The residuals should be approximately normally distributed. This assumption is not crucial for large sample sizes due to the Central Limit Theorem.we can check normality using a histogram of residuals, a Q-Q plot, or statistical tests .\n",
    "\n",
    "##### To check these assumptions:\n",
    "\n",
    "Residual Analysis: Examine the residuals by plotting them against the predicted values. Look for patterns or trends in the residuals, which could indicate violations of assumptions.\n",
    "\n",
    "Normality Tests: Use statistical tests like the Shapiro-Wilk test or visual inspection of the residuals through histograms and Q-Q plots.\n",
    "\n",
    "Scatterplots: For simple linear regression, examine scatterplots of the dependent variable against each independent variable to assess linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b826666b",
   "metadata": {},
   "source": [
    "#### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c43101",
   "metadata": {},
   "source": [
    " In a linear regression model of the form Y=aX+b where Y is the dependent variable, X is the independent variable, a is the intercept,b is the slope.\n",
    "\n",
    "slope: with the unit moment of x-asis ,how much unit moment in Y-axis\n",
    "\n",
    "intercept:what is the value of Y at x=0.\n",
    "\n",
    "EX:\n",
    "PREDICTING HOUSE PRICE\n",
    "\n",
    "suppose we have a dataset with X is the size of the house and Y is the price\n",
    "\n",
    "house price=a+b*house size\n",
    "\n",
    "intercept(a):\n",
    "      If the intercept is $50,000, it means that, according to the model, a house with zero square feet (which is not practically meaningful) is estimated to have a price of $50,000. In this context, the intercept might include fixed costs like land value or other baseline costs.\n",
    "slope (b):\n",
    "     If the slope is 150, it means that, on average, each additional square foot is associated with an increase of $150 in the house price. So, if a house is 1,000 square feet larger than another, you would expect it to be priced $150,000 higher, all else being equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39239fdd",
   "metadata": {},
   "source": [
    "#### Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623893c0",
   "metadata": {},
   "source": [
    "#### Gradient Descent:\n",
    "Gradient descent is an iterative optimization algorithm used for finding the minimum of a function. In the context of machine learning, this function is often a cost or loss function that measures the difference between the predicted values of a model and the actual values.\n",
    "\n",
    "θ=θ−α∇J(θ)\n",
    "\n",
    "θ is the parameter vector (weights and biases), α is the learning rate, and ∇J(θ) is the gradient of the cost function with respect to θ.\n",
    "\n",
    "#### Key Components of Gradient Descent:\n",
    "\n",
    "#### Learning Rate (α):\n",
    "The learning rate determines the size of the steps taken in the parameter space during each iteration. Too small a learning rate can lead to slow convergence, while too large a learning rate may cause overshooting and divergence.\n",
    "\n",
    "#### Cost Function:\n",
    "The cost function quantifies how well the model's predictions match the actual values. The goal is to minimize this function.\n",
    "\n",
    "#### Convergence Criteria:\n",
    "The algorithm stops when a certain convergence criterion is met, such as reaching a specified number of iterations or when the change in the cost function becomes negligible.\n",
    "\n",
    "Gradient descent is a fundamental optimization algorithm and is widely used in training machine learning models, especially in the context of training neural networks. It helps find optimal parameters that minimize the difference between predicted and actual values, making the model more accurate and effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb8222f",
   "metadata": {},
   "source": [
    "#### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693d77d9",
   "metadata": {},
   "source": [
    "#### Multiple Linear Regression:\n",
    "            Multiple linear regression is an extension of simple linear regression that allows for modeling the relationship between a dependent variable (Y) and multiple independent variables .(X1,X2,X3....).General form is:\n",
    "            Y=a+bX1+cX2+.....+nXn\n",
    "            \n",
    "#### Differences between simple linear Regression and Multiple linear regression:\n",
    "#### 1.Simple Linear Regression:\n",
    "It is used to model a relationship between two continuous variables.i.e;one is input variable and the other is output variable.\n",
    "\n",
    "EX:Suppose a company wants to determine relation between number of hours employee spend on training and spend on work.\n",
    "\n",
    "#### 2.Multiple Linear Regression:\n",
    "It is used to find mathematical relationship between several random variables .i.e;multiple independent variables and single output variable.\n",
    "\n",
    "EX:Suppose to determine a price of house we need two features i.e;area of house ,number of rooms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fec755",
   "metadata": {},
   "source": [
    "#### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beafa08",
   "metadata": {},
   "source": [
    "Multicollinearity is a statistical phenomenon in multiple linear regression where two or more independent variables are highly correlated, making it difficult to isolate the individual effect of each variable on the dependent variable.\n",
    "\n",
    "#### Effects of Multicollinearity:\n",
    "#### Unreliable Coefficients:\n",
    "The estimated coefficients become unstable and may have large standard errors. Loss of Precision: It becomes difficult to precisely estimate the contribution of each variable to the dependent variable.\n",
    "\n",
    "#### Interpretation Issues:\n",
    "Interpretation of individual variable effects becomes problematic.\n",
    "#### Reduced Statistical Power:\n",
    "The ability to detect the true effects of variables is diminished\n",
    "\n",
    "#### Detecting Multicollinearity:\n",
    "#### Correlation Matrix: \n",
    "Examine the correlation matrix among the independent variables. High correlation coefficients (close to +1 or -1) indicate potential multicollinearity.\n",
    "\n",
    "#### Variance Inflation Factor (VIF): \n",
    "Calculate the VIF for each independent variable. VIF measures how much the variance of an estimated regression coefficient increases if the predictors are correlated. A high VIF (usually above 10) suggests multicollinearity.\n",
    "\n",
    "#### Tolerance:\n",
    "Tolerance is the reciprocal of VIF. A low tolerance (close to 0) indicates high multicollinearity.\n",
    "\n",
    "#### Addressing Multicollinearity:\n",
    "#### Remove Highly Correlated Variables: \n",
    "If two or more variables are highly correlated, consider removing one of them from the model.\n",
    "\n",
    "####  Selection:\n",
    "Use feature selection techniques to choose a subset of relevant variables and exclude highly correlated variables.\n",
    "\n",
    "#### Combine Variables: \n",
    "If it makes sense in the context of the problem, create new variables that are combinations of the highly correlated variables.\n",
    "\n",
    "#### Ridge Regression or LASSO Regression: \n",
    "These regularization techniques can help mitigate the impact of multicollinearity by adding a penalty term to the regression coefficients.\n",
    "\n",
    "#### Principal Component Analysis (PCA): \n",
    "Transform the original variables into a new set of uncorrelated variables (principal components) using PCA.\n",
    "\n",
    "#### Collect More Data: \n",
    "Increasing the sample size can sometimes alleviate multicollinearity issues.\n",
    "\n",
    "#### Centering Variables: \n",
    "Centering (subtracting the mean) of variables can sometimes help, especially when interactions are involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fae5d9",
   "metadata": {},
   "source": [
    "#### Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722c8e7b",
   "metadata": {},
   "source": [
    "#### Polynomial Regression model:\n",
    "            It is used to model a relationship between one or more independent features and one dependent feature. here it is only helpful for Non linear Regression.\n",
    "Polynomial regression is different from Linear Regression as linear Regression has only one independent and one dependent feature with a linear relation between the variables.\n",
    "\n",
    "where as Polynomial Regression has one or more independent and one dependent feature with a non linear relationship between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734b97bd",
   "metadata": {},
   "source": [
    "#### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02e7098",
   "metadata": {},
   "source": [
    "#### Advantages of Polynomial Regression:\n",
    "Flexibility in Modeling Nonlinear Relationships:\n",
    "Polynomial regression can capture nonlinear relationships between the independent and dependent variables. Linear regression is limited to modeling linear relationships, and polynomial regression provides more flexibility in representing curved patterns.\n",
    "\n",
    "Better Fit to Complex Data:\n",
    "In situations where the true relationship between variables is nonlinear, polynomial regression can provide a better fit than linear regression. It allows the model to adapt to the curvature of the data.\n",
    "\n",
    "Higher Order Polynomial Models:\n",
    "By introducing higher-order terms (quadratic, cubic, etc.), polynomial regression can account for more complex patterns and capture intricate structures in the data.\n",
    "\n",
    "#### Disadvantages of Polynomial Regression:\n",
    "Overfitting:\n",
    "Using higher-order polynomials increases the risk of overfitting, where the model fits the training data too closely and performs poorly on new, unseen data. This is especially true when the degree of the polynomial is too high relative to the amount of available data.\n",
    "\n",
    "Increased Complexity:\n",
    "Higher-order polynomials introduce more parameters, making the model more complex. This complexity can lead to difficulties in interpretation and may result in a less parsimonious model.\n",
    "\n",
    "Sensitivity to Outliers:\n",
    "Polynomial regression can be sensitive to outliers, as the model tries to fit the data, including outliers, which might not represent the underlying pattern.\n",
    "\n",
    "#### When to Use Polynomial Regression:\n",
    "Nonlinear Relationships:\n",
    "When the relationship between the dependent and independent variables is expected to be nonlinear, polynomial regression can be a suitable choice.\n",
    "\n",
    "Complex Patterns:\n",
    "In cases where the data exhibits complex patterns or curvature, polynomial regression allows for a more accurate representation of the underlying structure.\n",
    "\n",
    "Exploratory Analysis:\n",
    "Polynomial regression can be useful in exploratory data analysis when you are unsure about the nature of the relationship and want to allow the model to adapt to different patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c279c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
